{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "WGoorsK6mPZP"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torchvision as tv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BQEb43_PmPZk"
   },
   "source": [
    "## Данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "qb_NW7cVmPZs"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE=256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kw5Kv2i6mPZu",
    "outputId": "abec58e9-6ac4-4622-af32-8d4ddc849f3d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset = tv.datasets.FashionMNIST('.', train=True, transform=tv.transforms.ToTensor(), download=True)\n",
    "test_dataset = tv.datasets.FashionMNIST('.', train=False, transform=tv.transforms.ToTensor(), download=True)\n",
    "train = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "test = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BQEb43_PmPZk"
   },
   "source": [
    "## Функция для обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "vVqz1hvTmPaA"
   },
   "outputs": [],
   "source": [
    "def train_model(num_epochs):\n",
    "    for ep in range(num_epochs):\n",
    "        train_iters, train_passed = 0, 0\n",
    "        train_loss, train_acc = 0., 0.\n",
    "        start = time.time()\n",
    "        \n",
    "        model.train()\n",
    "        for X, y in train:\n",
    "            trainer.zero_grad()\n",
    "            y_pred = model(X)\n",
    "            l = loss(y_pred, y)\n",
    "            l.backward()\n",
    "            trainer.step()\n",
    "            train_loss += l.item()\n",
    "            train_acc += (y_pred.argmax(dim=1) == y).sum().item()\n",
    "            train_iters += 1\n",
    "            train_passed += len(X)\n",
    "        \n",
    "        test_iters, test_passed = 0, 0\n",
    "        test_loss, test_acc = 0., 0.\n",
    "        model.eval()\n",
    "        for X, y in test:\n",
    "            y_pred = model(X)\n",
    "            l = loss(y_pred, y)\n",
    "            test_loss += l.item()\n",
    "            test_acc += (y_pred.argmax(dim=1) == y).sum().item()\n",
    "            test_iters += 1\n",
    "            test_passed += len(X)\n",
    "            \n",
    "        print(\"ep: {}, taked: {:.3f}, train_loss: {:.3f}, train_acc: {:.3f}, test_loss: {:.3f}, test_acc: {:.3f}\".format(\n",
    "            ep, time.time() - start, train_loss / train_iters, train_acc / train_passed,\n",
    "            test_loss / test_iters, test_acc / test_passed)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-LcgNXwOmPZ5"
   },
   "source": [
    "## Модель 1\n",
    " - 1 скрытый слой на 256 нейронов\n",
    " - Оптимизатор Adam\n",
    " - Обучение 20 эпох"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "V7qWC2EbmPZ5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 0, taked: 7.417, train_loss: 0.620, train_acc: 0.790, test_loss: 0.494, test_acc: 0.822\n",
      "ep: 1, taked: 8.011, train_loss: 0.428, train_acc: 0.850, test_loss: 0.441, test_acc: 0.844\n",
      "ep: 2, taked: 7.867, train_loss: 0.384, train_acc: 0.864, test_loss: 0.409, test_acc: 0.855\n",
      "ep: 3, taked: 7.913, train_loss: 0.356, train_acc: 0.873, test_loss: 0.388, test_acc: 0.862\n",
      "ep: 4, taked: 9.105, train_loss: 0.334, train_acc: 0.880, test_loss: 0.372, test_acc: 0.868\n",
      "ep: 5, taked: 11.164, train_loss: 0.317, train_acc: 0.886, test_loss: 0.362, test_acc: 0.869\n",
      "ep: 6, taked: 8.568, train_loss: 0.303, train_acc: 0.891, test_loss: 0.355, test_acc: 0.873\n",
      "ep: 7, taked: 7.954, train_loss: 0.291, train_acc: 0.895, test_loss: 0.351, test_acc: 0.873\n",
      "ep: 8, taked: 7.945, train_loss: 0.280, train_acc: 0.899, test_loss: 0.349, test_acc: 0.874\n",
      "ep: 9, taked: 7.940, train_loss: 0.269, train_acc: 0.903, test_loss: 0.344, test_acc: 0.877\n",
      "ep: 10, taked: 8.042, train_loss: 0.260, train_acc: 0.906, test_loss: 0.342, test_acc: 0.877\n",
      "ep: 11, taked: 7.943, train_loss: 0.252, train_acc: 0.910, test_loss: 0.342, test_acc: 0.877\n",
      "ep: 12, taked: 7.859, train_loss: 0.244, train_acc: 0.913, test_loss: 0.339, test_acc: 0.878\n",
      "ep: 13, taked: 11.299, train_loss: 0.237, train_acc: 0.915, test_loss: 0.341, test_acc: 0.879\n",
      "ep: 14, taked: 9.611, train_loss: 0.230, train_acc: 0.917, test_loss: 0.340, test_acc: 0.880\n",
      "ep: 15, taked: 7.895, train_loss: 0.224, train_acc: 0.920, test_loss: 0.339, test_acc: 0.880\n",
      "ep: 16, taked: 7.944, train_loss: 0.218, train_acc: 0.923, test_loss: 0.345, test_acc: 0.881\n",
      "ep: 17, taked: 7.967, train_loss: 0.212, train_acc: 0.924, test_loss: 0.345, test_acc: 0.880\n",
      "ep: 18, taked: 9.009, train_loss: 0.207, train_acc: 0.927, test_loss: 0.344, test_acc: 0.881\n",
      "ep: 19, taked: 10.835, train_loss: 0.202, train_acc: 0.928, test_loss: 0.351, test_acc: 0.881\n"
     ]
    }
   ],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(784, 256),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(256, 10)\n",
    ")\n",
    "\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "trainer = torch.optim.Adam(model.parameters())\n",
    "train_model(num_epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Модель 1 достигла точности на тестовой выборке 88% после прохождения 15 эпох\n",
    " - Лучшая эпоха по тестовой выборке: `ep: 15, test_loss: 0.339, test_acc: 0.880`\n",
    " - Лучшая модель: Модель 1 `ep: 15, test_loss: 0.339, test_acc: 0.880`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "urhJrVvSmPaF"
   },
   "source": [
    "## Модель 2\n",
    " - 3 скрытых слоя на 512, 256 и 128 нейронов\n",
    " - Оптимизатор Adam\n",
    " - Обучение 20 эпох"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "lqYmS2z6mPaG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 0, taked: 10.045, train_loss: 0.631, train_acc: 0.778, test_loss: 0.465, test_acc: 0.831\n",
      "ep: 1, taked: 12.781, train_loss: 0.402, train_acc: 0.856, test_loss: 0.401, test_acc: 0.857\n",
      "ep: 2, taked: 12.082, train_loss: 0.351, train_acc: 0.872, test_loss: 0.384, test_acc: 0.861\n",
      "ep: 3, taked: 12.079, train_loss: 0.320, train_acc: 0.882, test_loss: 0.377, test_acc: 0.864\n",
      "ep: 4, taked: 10.752, train_loss: 0.298, train_acc: 0.890, test_loss: 0.377, test_acc: 0.867\n",
      "ep: 5, taked: 11.868, train_loss: 0.279, train_acc: 0.897, test_loss: 0.370, test_acc: 0.871\n",
      "ep: 6, taked: 13.147, train_loss: 0.264, train_acc: 0.902, test_loss: 0.370, test_acc: 0.871\n",
      "ep: 7, taked: 10.108, train_loss: 0.254, train_acc: 0.905, test_loss: 0.346, test_acc: 0.876\n",
      "ep: 8, taked: 12.062, train_loss: 0.242, train_acc: 0.910, test_loss: 0.342, test_acc: 0.879\n",
      "ep: 9, taked: 10.238, train_loss: 0.230, train_acc: 0.914, test_loss: 0.355, test_acc: 0.878\n",
      "ep: 10, taked: 11.476, train_loss: 0.221, train_acc: 0.918, test_loss: 0.363, test_acc: 0.876\n",
      "ep: 11, taked: 11.003, train_loss: 0.215, train_acc: 0.921, test_loss: 0.350, test_acc: 0.880\n",
      "ep: 12, taked: 11.300, train_loss: 0.205, train_acc: 0.923, test_loss: 0.353, test_acc: 0.878\n",
      "ep: 13, taked: 11.753, train_loss: 0.196, train_acc: 0.926, test_loss: 0.358, test_acc: 0.883\n",
      "ep: 14, taked: 10.803, train_loss: 0.189, train_acc: 0.928, test_loss: 0.338, test_acc: 0.887\n",
      "ep: 15, taked: 14.155, train_loss: 0.184, train_acc: 0.930, test_loss: 0.369, test_acc: 0.876\n",
      "ep: 16, taked: 10.187, train_loss: 0.176, train_acc: 0.934, test_loss: 0.365, test_acc: 0.877\n",
      "ep: 17, taked: 12.788, train_loss: 0.171, train_acc: 0.936, test_loss: 0.360, test_acc: 0.886\n",
      "ep: 18, taked: 10.932, train_loss: 0.164, train_acc: 0.937, test_loss: 0.368, test_acc: 0.885\n",
      "ep: 19, taked: 11.788, train_loss: 0.158, train_acc: 0.939, test_loss: 0.377, test_acc: 0.886\n"
     ]
    }
   ],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(784, 512),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(512, 256),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(256, 128),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128, 10)\n",
    ")\n",
    "\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "trainer = torch.optim.Adam(model.parameters())\n",
    "train_model(num_epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Модель 2 достигла точности на тестовой выборке 88% после прохождения 12 эпох\n",
    " - Лучшая эпоха по тестовой выборке: `ep: 14, test_loss: 0.338, test_acc: 0.887`\n",
    " - Лучшая модель: Модель 2 `ep: 14, test_loss: 0.338, test_acc: 0.887`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MMELAZ3dmPaI"
   },
   "source": [
    "## Модель 3\n",
    " - 3 скрытых слоя на 512, 256 и 128 нейронов\n",
    " - В каждом скрытом слое нормализация по батчу перед активацией\n",
    " - Оптимизатор Adam\n",
    " - Обучение 20 эпох"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "bW8thRZnmPaI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 0, taked: 11.443, train_loss: 0.464, train_acc: 0.842, test_loss: 0.394, test_acc: 0.853\n",
      "ep: 1, taked: 11.468, train_loss: 0.318, train_acc: 0.883, test_loss: 0.385, test_acc: 0.858\n",
      "ep: 2, taked: 11.417, train_loss: 0.276, train_acc: 0.898, test_loss: 0.383, test_acc: 0.862\n",
      "ep: 3, taked: 10.254, train_loss: 0.247, train_acc: 0.910, test_loss: 0.373, test_acc: 0.863\n",
      "ep: 4, taked: 11.406, train_loss: 0.221, train_acc: 0.919, test_loss: 0.359, test_acc: 0.871\n",
      "ep: 5, taked: 11.220, train_loss: 0.202, train_acc: 0.925, test_loss: 0.371, test_acc: 0.867\n",
      "ep: 6, taked: 11.775, train_loss: 0.185, train_acc: 0.932, test_loss: 0.361, test_acc: 0.875\n",
      "ep: 7, taked: 14.427, train_loss: 0.169, train_acc: 0.937, test_loss: 0.392, test_acc: 0.872\n",
      "ep: 8, taked: 10.445, train_loss: 0.154, train_acc: 0.943, test_loss: 0.367, test_acc: 0.877\n",
      "ep: 9, taked: 13.074, train_loss: 0.141, train_acc: 0.948, test_loss: 0.383, test_acc: 0.878\n",
      "ep: 10, taked: 10.414, train_loss: 0.128, train_acc: 0.953, test_loss: 0.422, test_acc: 0.872\n",
      "ep: 11, taked: 14.420, train_loss: 0.118, train_acc: 0.957, test_loss: 0.404, test_acc: 0.880\n",
      "ep: 12, taked: 11.283, train_loss: 0.109, train_acc: 0.960, test_loss: 0.467, test_acc: 0.871\n",
      "ep: 13, taked: 10.480, train_loss: 0.101, train_acc: 0.962, test_loss: 0.440, test_acc: 0.879\n",
      "ep: 14, taked: 14.024, train_loss: 0.092, train_acc: 0.966, test_loss: 0.502, test_acc: 0.870\n",
      "ep: 15, taked: 10.724, train_loss: 0.091, train_acc: 0.966, test_loss: 0.464, test_acc: 0.877\n",
      "ep: 16, taked: 12.904, train_loss: 0.090, train_acc: 0.965, test_loss: 0.519, test_acc: 0.871\n",
      "ep: 17, taked: 10.594, train_loss: 0.078, train_acc: 0.970, test_loss: 0.561, test_acc: 0.867\n",
      "ep: 18, taked: 14.860, train_loss: 0.072, train_acc: 0.973, test_loss: 0.541, test_acc: 0.870\n",
      "ep: 19, taked: 10.455, train_loss: 0.064, train_acc: 0.976, test_loss: 0.552, test_acc: 0.874\n"
     ]
    }
   ],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(784, 512),\n",
    "    torch.nn.BatchNorm1d(512),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(512, 256),\n",
    "    torch.nn.BatchNorm1d(256),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(256, 128),\n",
    "    torch.nn.BatchNorm1d(128),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128, 10)\n",
    ")\n",
    "\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "trainer = torch.optim.Adam(model.parameters())\n",
    "train_model(num_epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Модель 3 достигла точности на тестовой выборке 88% после прохождения 12 эпох\n",
    " - Лучшая эпоха по тестовой выборке: `ep: 4, test_loss: 0.359, test_acc: 0.871`\n",
    " - Лучшая модель: Модель 2 `ep: 14, test_loss: 0.338, test_acc: 0.887`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MMELAZ3dmPaI"
   },
   "source": [
    "## Модель 4\n",
    " - 3 скрытых слоя на 512, 256 и 128 нейронов\n",
    " - В каждом скрытом слое нормализация по батчу перед активацией и дропаут после активации\n",
    " - Оптимизатор Adam\n",
    " - Обучение 30 эпох"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "bW8thRZnmPaI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 0, taked: 10.584, train_loss: 0.704, train_acc: 0.769, test_loss: 0.441, test_acc: 0.840\n",
      "ep: 1, taked: 11.793, train_loss: 0.470, train_acc: 0.839, test_loss: 0.400, test_acc: 0.848\n",
      "ep: 2, taked: 11.975, train_loss: 0.425, train_acc: 0.852, test_loss: 0.373, test_acc: 0.862\n",
      "ep: 3, taked: 11.533, train_loss: 0.399, train_acc: 0.860, test_loss: 0.365, test_acc: 0.866\n",
      "ep: 4, taked: 13.451, train_loss: 0.380, train_acc: 0.866, test_loss: 0.355, test_acc: 0.868\n",
      "ep: 5, taked: 11.542, train_loss: 0.365, train_acc: 0.872, test_loss: 0.356, test_acc: 0.866\n",
      "ep: 6, taked: 11.589, train_loss: 0.353, train_acc: 0.876, test_loss: 0.341, test_acc: 0.874\n",
      "ep: 7, taked: 11.745, train_loss: 0.341, train_acc: 0.878, test_loss: 0.342, test_acc: 0.871\n",
      "ep: 8, taked: 19.658, train_loss: 0.332, train_acc: 0.880, test_loss: 0.326, test_acc: 0.881\n",
      "ep: 9, taked: 10.899, train_loss: 0.327, train_acc: 0.884, test_loss: 0.326, test_acc: 0.878\n",
      "ep: 10, taked: 19.338, train_loss: 0.315, train_acc: 0.888, test_loss: 0.317, test_acc: 0.884\n",
      "ep: 11, taked: 15.689, train_loss: 0.308, train_acc: 0.890, test_loss: 0.317, test_acc: 0.882\n",
      "ep: 12, taked: 10.631, train_loss: 0.303, train_acc: 0.892, test_loss: 0.321, test_acc: 0.879\n",
      "ep: 13, taked: 28.989, train_loss: 0.295, train_acc: 0.894, test_loss: 0.317, test_acc: 0.885\n",
      "ep: 14, taked: 10.533, train_loss: 0.289, train_acc: 0.897, test_loss: 0.315, test_acc: 0.884\n",
      "ep: 15, taked: 21.032, train_loss: 0.286, train_acc: 0.898, test_loss: 0.319, test_acc: 0.883\n",
      "ep: 16, taked: 10.723, train_loss: 0.280, train_acc: 0.900, test_loss: 0.310, test_acc: 0.890\n",
      "ep: 17, taked: 19.925, train_loss: 0.276, train_acc: 0.901, test_loss: 0.309, test_acc: 0.886\n",
      "ep: 18, taked: 10.591, train_loss: 0.271, train_acc: 0.903, test_loss: 0.318, test_acc: 0.886\n",
      "ep: 19, taked: 25.221, train_loss: 0.264, train_acc: 0.904, test_loss: 0.310, test_acc: 0.890\n",
      "ep: 20, taked: 11.399, train_loss: 0.262, train_acc: 0.905, test_loss: 0.316, test_acc: 0.886\n",
      "ep: 21, taked: 14.786, train_loss: 0.256, train_acc: 0.907, test_loss: 0.318, test_acc: 0.884\n",
      "ep: 22, taked: 22.818, train_loss: 0.251, train_acc: 0.909, test_loss: 0.306, test_acc: 0.890\n",
      "ep: 23, taked: 11.332, train_loss: 0.250, train_acc: 0.911, test_loss: 0.309, test_acc: 0.889\n",
      "ep: 24, taked: 17.328, train_loss: 0.242, train_acc: 0.912, test_loss: 0.305, test_acc: 0.892\n",
      "ep: 25, taked: 10.626, train_loss: 0.239, train_acc: 0.913, test_loss: 0.300, test_acc: 0.892\n",
      "ep: 26, taked: 18.471, train_loss: 0.235, train_acc: 0.915, test_loss: 0.309, test_acc: 0.890\n",
      "ep: 27, taked: 12.172, train_loss: 0.233, train_acc: 0.917, test_loss: 0.301, test_acc: 0.893\n",
      "ep: 28, taked: 21.972, train_loss: 0.228, train_acc: 0.917, test_loss: 0.296, test_acc: 0.897\n",
      "ep: 29, taked: 11.459, train_loss: 0.229, train_acc: 0.917, test_loss: 0.298, test_acc: 0.895\n"
     ]
    }
   ],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(784, 512),\n",
    "    torch.nn.BatchNorm1d(512),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(0.5),\n",
    "    torch.nn.Linear(512, 256),\n",
    "    torch.nn.BatchNorm1d(256),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(0.5),\n",
    "    torch.nn.Linear(256, 128),\n",
    "    torch.nn.BatchNorm1d(128),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(0.5),\n",
    "    torch.nn.Linear(128, 10)\n",
    ")\n",
    "\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "trainer = torch.optim.Adam(model.parameters())\n",
    "train_model(num_epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Модель 4 достигла точности на тестовой выборке 88% после прохождения 9 эпох\n",
    " - Лучшая эпоха по тестовой выборке: `ep: 28, test_loss: 0.296, test_acc: 0.897`\n",
    " - Лучшая модель: Модель 4 `ep: 28, test_loss: 0.296, test_acc: 0.897`\n",
    "\n",
    "Модель 4 уже уверенно перешагнула порог точности 88%, приблизившись к 90%. Судя по логу процесса оптимизации Модель 4 могла бы продолжить обучение дольше 30 эпох, сходясь к еще более качественному решению."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
